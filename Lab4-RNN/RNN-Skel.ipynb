{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfIdnkh92358"
   },
   "source": [
    "# RNN training tutorial\n",
    "### Adapted from the EEML2019 Tutorial on RNNs\n",
    "\n",
    "The objective is to analyze the training of various RNNs on simple datasets and doing some analysis.\n",
    "\n",
    "Structure:\n",
    "\n",
    "  1. basic (vanilla RNN) implementation\n",
    "  2. observing exploding/vanishing gradients\n",
    "  \n",
    " Homework Assignment\n",
    "  3. Training an RNN on character level langugage modelling task (e.g. Shakespeare Sonnet dataset) - char RNN\n",
    "    * find an existing implementation, and adapt it to use LSTM cells and Vanilla RNN cells, playing with architectures  \n",
    "\n",
    "Look for the questions after Sections 1 and 2. Provide your answers during the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MA_K3_OL3EY4"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "j5YGV2hb2RIt"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "  \n",
    "sns.set_style('ticks')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0KPZdiq5AVJ"
   },
   "source": [
    "# Ex 1.    Vanilla RNN\n",
    "\n",
    "Implement basic RNN cell using tf.layers.\n",
    "\n",
    "   $$ h_t = f( Wx_t + Vh_{t-1}  + b) $$\n",
    "   \n",
    "   Where\n",
    "   \n",
    "   * $x_t$ input at time $t$\n",
    "   * $h_t$ hidden state at time $t$\n",
    "   * $W$ input-to-hidden mapping (trainable)\n",
    "   * $V$ hidden-to-hidden mapping (trainable)\n",
    "   * $b$ bias (trainable)\n",
    "   * $f$ non-linearity chosen (usually tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 Implement Vanilla RNN Base recurrence model\n",
    "$h_t = f(input + V h_(t-1) + b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "z6GIHgOnzd8Y"
   },
   "outputs": [],
   "source": [
    "class VanillaRNNBase(nn.Module):\n",
    "  \n",
    "    def __init__(self, hidden_size, activation=nn.Tanh, bias=True):    \n",
    "        \"\"\"\n",
    "        Constructor for a simple RNNCell where the hidden-to-hidden transitions\n",
    "        are defined by a linear layer and the default activation of `tanh` \n",
    "        :param hidden_size: the size of the hidden state\n",
    "        :param activation: the activation function used for computing the next hidden state\n",
    "        \"\"\"\n",
    "        super(VanillaRNNBase, self).__init__()\n",
    "    \n",
    "        self._hidden_size = hidden_size\n",
    "        self._activation = activation()  \n",
    "        self._bias = bias\n",
    "            \n",
    "        # TODO 1.1 Create the hidden-to-hidden layer\n",
    "        # self._linear_hh = nn.Linear(...)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        out = inputs\n",
    "        #### TODO 1.1 Your code here\n",
    "        ### ...\n",
    "        #### end code\n",
    "        \n",
    "        return out, out\n",
    "\n",
    "\n",
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, output_size=1, hidden_size=20, bias=False):\n",
    "        \"\"\"\n",
    "        Creates a vanilla RNN where input-to-hidden is a nn.Linear layer\n",
    "        and hidden-to-output is a nn.Linear layer\n",
    "        \n",
    "        :param input_size: the size of the input to the RNN\n",
    "        :param hidden_size: size of the hidden state of the RNN\n",
    "        :param output_size: size of the output\n",
    "        \"\"\"\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        \n",
    "        self._input_size = input_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._output_size = output_size\n",
    "        self._bias = bias\n",
    "        \n",
    "        self.in_to_hidden = nn.Linear(self._input_size, self._hidden_size, bias=self._bias)\n",
    "        self.rnn_cell = VanillaRNNBase(self._hidden_size, bias=self._bias)\n",
    "        self.hidden_to_out = nn.Linear(self._hidden_size, self._output_size, bias=self._bias)\n",
    "    \n",
    "    def step(self, input, hidden=None):\n",
    "        ### TODO 1.2 compute one step in the RNN\n",
    "        ## input_ = ....\n",
    "        ## _, hidden_ =  ....\n",
    "        # output_ = ....\n",
    "        \n",
    "        # return output_, hidden_\n",
    "        pass\n",
    "    \n",
    "    def forward(self, inputs, hidden=None, force=True, warm_start=10):\n",
    "        steps = len(inputs)\n",
    "        \n",
    "        outputs = torch.autograd.Variable(torch.zeros(steps, self._output_size, self._output_size))\n",
    "        \n",
    "        output_ = None\n",
    "        hidden_ = hidden\n",
    "        \n",
    "        for i in range(steps):\n",
    "            ## TODO 1.3 Implement forward pass in RNN\n",
    "            ## Implement Teacher Forcing and Warm Start\n",
    "            input_ = None\n",
    "            \n",
    "            ### END Code\n",
    "            \n",
    "            output_, hidden_ = self.step(input_, hidden_)\n",
    "            outputs[i] = output_\n",
    "            \n",
    "        return outputs, hidden_\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-jCR9YGaI7my"
   },
   "source": [
    "## Train RNN on sine wave\n",
    "\n",
    "Train the RNN on sine data - predict the next sine value from *predicted* sine values.\n",
    "\n",
    "Predict   $$ sin (x +t \\epsilon) $$ from $$ sin (x), sin (x + \\epsilon), ..., sin (x + (t-1) \\epsilon) $$\n",
    "\n",
    "In particular, we want the network to predict the next value in a loop, conditioning the prediction on some initial values (provided) and all subsequent predictions.\n",
    "\n",
    "To learn the prediction model, we will use *teacher forcing*. This means that when training the model, the input at time $t$ is the real sequence at time $t$, rather than the output produced by the model at $t-1$.\n",
    "\n",
    "When we want to generate data from the model, we do not have access to the true sequence, so we do not use teacher forcing. However, in the case of our problem, we will also use *warm starting*, because we require multiple time steps to predict the next sine wave value (at least 2, for the initial value and for the step). \n",
    "\n",
    "The code below unrolls the RNN core you have defined above, does the training using backprop though time and plots the real data (\"ground truth\"), the data generated during training (\"train predictions\") and the model samples \"generated\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f'Running code @ {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNROLL_LENGTH = 30  #@param {type:\"integer\"}\n",
    "NUM_ITERATIONS = 10000  #@param {type:\"integer\"}\n",
    "WARM_START = 10  #@param {type:\"integer\"}\n",
    "TEACHER_FORCING = False  #@param {type:\"boolean\"}\n",
    "HIDDEN_UNITS = 20  #@param {type:\"integer\"}\n",
    "LEARNING_RATE = 0.0001  #@param {type:\"number\"}\n",
    "REPORTING_INTERVAL = 200  #@param {type:\"integer\"}\n",
    "\n",
    "# We create training data, sine wave over [0, 2pi]\n",
    "x_train = np.arange(0, 2*np.pi, 0.1).reshape(-1, 1, 1)\n",
    "y_train = np.sin(x_train)\n",
    "\n",
    "net = VanillaRNN(hidden_size=HIDDEN_UNITS, bias=False)\n",
    "net.train()\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "running_loss = 0\n",
    "\n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "    # select a start point in the training set for a sequence of UNROLL_LENGTH\n",
    "    start = np.random.choice(range(x_train.shape[0] - UNROLL_LENGTH))\n",
    "    train_sequence = y_train[start : (start + UNROLL_LENGTH)]\n",
    "    \n",
    "    train_inputs = torch.from_numpy(train_sequence[:-1]).float().to(device)\n",
    "    train_targets = torch.from_numpy(train_sequence[1:]).float().to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs, hidden = net(train_inputs, hidden=None, force=TEACHER_FORCING, warm_start=WARM_START)\n",
    "    loss = criterion(outputs, train_targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if iteration % REPORTING_INTERVAL == REPORTING_INTERVAL - 1:\n",
    "        # let's see how well we do on predictions for the whole sequence\n",
    "        avg_loss = running_loss / REPORTING_INTERVAL\n",
    "        \n",
    "        report_sequence = torch.from_numpy(y_train[:-1]).float().to(device)\n",
    "        report_targets = torch.from_numpy(y_train[1:]).float().to(device)\n",
    "        report_output, report_hidden = net(report_sequence, hidden=None, force=False, warm_start=WARM_START)\n",
    "        \n",
    "        report_loss = criterion(report_output, report_targets)\n",
    "        print('[%d] avg_loss: %.5f, report_loss: %.5f, ' % (iteration + 1, avg_loss, report_loss.item()))\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.title('Training Loss %.5f;  Sampling loss %.5f; Iteration %d' % (avg_loss, report_loss.item(), iteration))\n",
    "        \n",
    "        plt.plot(y_train[1:].ravel(), c='blue', label='Ground truth',\n",
    "               linestyle=\":\", lw=6)\n",
    "        plt.plot(range(start, start+UNROLL_LENGTH-1), outputs.data.numpy().ravel(), c='gold',\n",
    "               label='Train prediction', lw=5, marker=\"o\", markersize=5,\n",
    "               alpha=0.7)\n",
    "        plt.plot(report_output.data.numpy().ravel(), c='r', label='Generated', lw=4, alpha=0.7)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gOKWVWE9sDke"
   },
   "outputs": [],
   "source": [
    "# Default hypers:\n",
    "# UNROLL_LENGTH = 30  #@param {type:\"integer\"}\n",
    "# NUM_ITERATIONS = 10000  #@param {type:\"integer\"}\n",
    "# WARM_START = 2  #@param {type:\"integer\"}\n",
    "# TEACHER_FORCING = False  #@param {type:\"boolean\"}\n",
    "# HIDDEN_UNITS = 20  #@param {type:\"integer\"}\n",
    "# LEARNING_RATE = 0.0001  #@param {type:\"number\"}\n",
    "# REPORTING_INTERVAL = 2000  #@param {type:\"integer\"}\n",
    "\n",
    "# You may want to try:\n",
    "# default hypers with/without teacher forcing\n",
    "# use UNROLL_LENGTH = 62 to train on the whole sequence (is teacher forcing useful?)\n",
    "# use UNROLL_LENGTH = 62, no teacher forcing and warm_start = 2 # this should break training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLuIAK8LJWay"
   },
   "source": [
    "**Note:** initialization is not fixed (we do not fix a random seed), so each time the cell is executed, the parameters take new initial values and hence training can lead to different results. What happens if you run it multiple times?\n",
    "\n",
    "###What is worth trying/understanding here?\n",
    "\n",
    "* Difference between teacher forcing and learning on own samples:\n",
    " * What are the pros and cons of teacher forcing?\n",
    " * Why is the model struggling to learn in one of the setups?\n",
    " * What is it we actually care about for models like this? What should be the actual surrogate?\n",
    "* How does warm starting affect our training? Why?\n",
    "* What happens if the structure of interest is much longer than the unroll length?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uxyUegmC5_Hj"
   },
   "source": [
    "# Ex. 2      Vanishing and exploding gradients\n",
    "\n",
    "Given an input sequence $(x_1, ..., x_N)$ of random floats (sampled from normal distribution), train an RNN as before and compute the gradients of the last output state w.r.t. every previous state:\n",
    "$$\n",
    "\\left \\| \\frac{\\partial h_{N}}{\\partial h_i} \\right \\|\n",
    "$$\n",
    "for each unroll $i$, and plot these quantities for various RNNs.\n",
    "\n",
    "Note, that during learning one would compute\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta}  \n",
    "$$\n",
    "which, using chain rule will involve terms like\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_N} \\cdot\n",
    "\\frac{\\partial h_N}{\\partial h_{N-1}} \\cdot\n",
    "\\dots \\cdot\n",
    "\\frac{\\partial h_i}{\\partial h_{i-1}} \\cdot\n",
    "\\dots \\cdot\n",
    "\\frac{\\partial h_0}{\\partial \\theta}\n",
    "$$\n",
    "so if one of them vanishes, all of them do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ULSWaWPtpynM"
   },
   "source": [
    "# Hints:\n",
    "\n",
    "PyTorch already defines many types of RNN Cells, such as LSTM, GRU, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAL8UB3QvoF7"
   },
   "source": [
    "NB: There is no training here, we are just computing the norms of the gradients of the last hidden state with respect to the hidden state across steps in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "zljqN01vc9-3"
   },
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 15  #@param {type:\"integer\"}\n",
    "HIDDEN_UNITS = 20  #@param {type:\"integer\"}\n",
    "\n",
    "dummy_input = [torch.from_numpy(np.array([[np.random.normal()]])) for _ in range(SEQ_LENGTH)] \n",
    "\n",
    "######################\n",
    "#   YOUR CODE HERE   #\n",
    "######################\n",
    "# Add several cell constructors (use those already defined in Tensorflow) to the\n",
    "# list (e.g., also add a GRU, and a few more LSTMS with their initial \n",
    "# forget_bias values set to: 0, +1, +2 and -2).\n",
    "# If in doubt, check the documentation.\n",
    "\n",
    "def _set_forget_bias(lstm_cell, fill_value=0.):\n",
    "    # The bias terms in the lstm_cell are arranged as bias_input_gate, bias_forget_gate, bias_gain_gate, bias_output_gate\n",
    "    # To alter the forget_gate bias, we need to modify the parameters from 1/4 to 1/2 of the length of the bias vectors\n",
    "    for name, _ in lstm_cell.named_parameters():\n",
    "        if \"bias\" in name:\n",
    "            bias = getattr(lstm_cell, name)\n",
    "            n = bias.size(0)\n",
    "            start, end = n//4, n//2\n",
    "            bias.data[start:end].fill_(float(fill_value))\n",
    "            \n",
    "    return lstm_cell\n",
    "\n",
    "\n",
    "### Solution\n",
    "rnn_types = {\n",
    "    'LSTM (0)': lambda nhid:  _set_forget_bias(nn.modules.LSTMCell(input_size=1, hidden_size=nhid), fill_value=0.),\n",
    "    ## TODO add several types of LSTM cells varying the forget gate bias - e.g. +1, -2, +2, +10\n",
    "    # 'LSTM (+1)':  ...\n",
    "    # 'LSTM (-2)': ...\n",
    "    # 'LSTM (+2)': ...\n",
    "    #'LSTM (+10)': ...\n",
    "    # add a GRUCell\n",
    "    # 'GRU': ...\n",
    "    # add our RNN module\n",
    "    'RNN': lambda nhid: VanillaRNN(input_size=1, hidden_size=nhid),\n",
    "}\n",
    "\n",
    "depths = {rnn_type: [] for rnn_type in rnn_types}\n",
    "grad_norms = {rnn_type: [] for rnn_type in rnn_types}\n",
    "\n",
    "for rnn_type in rnn_types:\n",
    "    \n",
    "    # build RNN model\n",
    "    constructor = rnn_types[rnn_type]\n",
    "    rnn = constructor(HIDDEN_UNITS)\n",
    "    \n",
    "    # initialize gradients\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    rnn_at_time = []\n",
    "    gradients_at_time = []\n",
    "    \n",
    "    prev_state = None\n",
    "    \n",
    "    # pass the sequence through the RNN model\n",
    "    for i in range(SEQ_LENGTH):\n",
    "        ## Each RNN cell model has a different output, so switch after the defined type\n",
    "        if prev_state is None:\n",
    "            prev_state = rnn(dummy_input[i].float())\n",
    "        else:\n",
    "            if rnn_type.startswith('RNN'):\n",
    "                prev_state = rnn(dummy_input[i].float(), hidden=prev_state[1])\n",
    "            else:\n",
    "                prev_state = rnn(dummy_input[i].float(), prev_state)\n",
    "        \n",
    "        ## We want to retain the gradient over the hidden state after each timestep (i.e. input of the sequence)\n",
    "        if rnn_type.startswith('LSTM'):\n",
    "            prev_state[1].retain_grad()  # for LSTMs the output is (h_t, c_t) . We call retain_grad() for c_t\n",
    "            rnn_at_time.append(prev_state[1])\n",
    "        \n",
    "        ## GRUs and our RNN model have only one \"hidden\" output - h_t\n",
    "        elif rnn_type.startswith('GRU'):\n",
    "            prev_state.retain_grad()\n",
    "            rnn_at_time.append(prev_state)\n",
    "        \n",
    "        elif rnn_type.startswith('RNN'):\n",
    "            prev_state[1].retain_grad()\n",
    "            rnn_at_time.append(prev_state[1])\n",
    "    \n",
    "    # We don't really care about the loss here: we are not solving a specific \n",
    "    # problem, any loss will work to inspect the behavior of the gradient.\n",
    "    dummy_loss = torch.sum(rnn_at_time[-1])\n",
    "    dummy_loss.backward()\n",
    "    \n",
    "    # collect all the gradients and plot them\n",
    "    for i in range(1, SEQ_LENGTH):\n",
    "        current_gradient = rnn_at_time[i].grad\n",
    "        gradients_at_time.append(current_gradient)\n",
    "    \n",
    "    for gid, grad in enumerate(gradients_at_time):\n",
    "        depths[rnn_type].append(len(gradients_at_time) - gid)    \n",
    "        grad_norms[rnn_type].append(np.linalg.norm(grad))\n",
    "        \n",
    "    dummy_loss.detach_()\n",
    "\n",
    "plt.figure()\n",
    "for rnn_type in depths:\n",
    "    plt.plot(depths[rnn_type], grad_norms[rnn_type], label=\"%s\" % rnn_type, alpha=0.7, lw=2)\n",
    "plt.legend()  \n",
    "plt.ylabel(\"$ \\\\| \\\\partial \\\\sum_i {c_{N}}_i / \\\\partial c_t \\\\|$\", fontsize=15)\n",
    "plt.xlabel(\"Steps through time - $t$\", fontsize=15)\n",
    "plt.xlim((1, SEQ_LENGTH-1))\n",
    "plt.title(\"Gradient magnitudes across time for: RNN-Type (forget_bias value)\")\n",
    "#plt.savefig(\"mygraph.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-4v7TUtjKHD-"
   },
   "source": [
    "### What do we learn from this?\n",
    "\n",
    "This particular experiment is an extremely simple surrogate for actual problem, but shows a few interesting aspects:\n",
    "\n",
    "* Is LSTM by construction free of *exploding* gradients too?\n",
    "* What are other ways of avoiding explosions you can think of?\n",
    "* Does initialisation (of gates here, but in general) matter a lot?\n",
    "\n",
    "See http://proceedings.mlr.press/v37/jozefowicz15.pdf for a more detailed discussion of the effect of the forget gate bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fpXpLbLVobgN"
   },
   "source": [
    "# Done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XwojS8kx0QyE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EEML2019_RNN_full.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
