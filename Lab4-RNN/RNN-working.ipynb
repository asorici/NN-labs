{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfIdnkh92358"
   },
   "source": [
    "# RNN training tutorial\n",
    "\n",
    "This is a tutorial training various RNNs on simple datasets and doing some analysis.\n",
    "\n",
    "Structure:\n",
    "\n",
    "  1. basic (vanilla RNN) implementation\n",
    "  2. observing exploding/vanishing gradients\n",
    "  3. training an LSTM on character level langugage modelling task\n",
    "    * comparing training of an LSTM and RNN, playing with architectures\n",
    "  4. Intepretability by plotting and analysing activations of a network:\n",
    "    * identifying interpretable neurons\n",
    "    * identifying neurons-gates interactions\n",
    "    * identifying hidden state dynamics through time\n",
    "  \n",
    "    \n",
    "\n",
    "First three sections are almost independent, one can go switch between them without any code dependencies (apart from being unable to use vanilla RNN in section 4, if it was not implemented in 1.).\n",
    "\n",
    "Cells that include \"starting point\" in their title require filling in some code gaps; all remaining ones are complete (but feel free to play with them if you want!)\n",
    "\n",
    "Please pay attention to questions after each section. Finding out answers to these is crucial to make sure one understands various modes of RNN operation.\n",
    "\n",
    "Language model exercises are based on [Sonnet LSTM example](https://github.com/deepmind/sonnet/blob/master/sonnet/examples/rnn_shakespeare.py).\n",
    "Apart from loading the dataset, we make no further use of Sonnet in this colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MA_K3_OL3EY4"
   },
   "source": [
    "## Imports\n",
    "\n",
    "We will use tf.nn.rnn_cell and tf.layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "j5YGV2hb2RIt"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# from sonnet.examples import dataset_shakespeare\n",
    "  \n",
    "sns.set_style('ticks')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0KPZdiq5AVJ"
   },
   "source": [
    "# Ex 1.    Vanilla RNN\n",
    "\n",
    "Implement basic RNN cell using tf.layers.\n",
    "\n",
    "   $$ h_t = f( Wx_t + Vh_{t-1}  + b) $$\n",
    "   \n",
    "   Where\n",
    "   \n",
    "   * $x_t$ input at time $t$\n",
    "   * $h_t$ hidden state at time $t$\n",
    "   * $W$ input-to-hidden mapping (trainable)\n",
    "   * $V$ hidden-to-hidden mapping (trainable)\n",
    "   * $b$ bias (trainable)\n",
    "   * $f$ non-linearity chosen (usually tanh)\n",
    "   \n",
    "   \n",
    "   You do not need to worry about the plotting and running code, but focus on the RNN implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "z6GIHgOnzd8Y"
   },
   "outputs": [],
   "source": [
    "#@title Vanilla RNN\n",
    "class VanillaRNNCell(nn.Module):\n",
    "  \n",
    "    def __init__(self, hidden_size, activation=nn.Tanh, bias=True):    \n",
    "        \"\"\"\n",
    "        Constructor for a simple RNNCell where the hidden-to-hidden transitions\n",
    "        are defined by a linear layer and the default activation of `tanh` \n",
    "        :param hidden_size: the size of the hidden state\n",
    "        :param activation: the activation function used for computing the next hidden state\n",
    "        \"\"\"\n",
    "        super(VanillaRNNCell, self).__init__()\n",
    "    \n",
    "        self._hidden_size = hidden_size\n",
    "        self._activation = activation()  \n",
    "        self._bias = bias\n",
    "            \n",
    "        # Create the hidden-to-hidden layer\n",
    "        self._linear_hh = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        out = inputs\n",
    "        \n",
    "        if hidden is not None:\n",
    "            out += self._linear_hh(hidden)\n",
    "        \n",
    "        out = self._activation(out)\n",
    "        return out, out\n",
    "\n",
    "\n",
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, output_size=1, hidden_size=20, bias=False):\n",
    "        \"\"\"\n",
    "        Creates a vanilla RNN where input-to-hidden is a nn.Linear layer\n",
    "        and hidden-to-output is a nn.Linear layer\n",
    "        \n",
    "        :param input_size: the size of the input to the RNN\n",
    "        :param hidden_size: size of the hidden state of the RNN\n",
    "        :param output_size: size of the output\n",
    "        \"\"\"\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        \n",
    "        self._input_size = input_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._output_size = output_size\n",
    "        self._bias = bias\n",
    "        \n",
    "        self.in_to_hidden = nn.Linear(self._input_size, self._hidden_size, bias=self._bias)\n",
    "        self.rnn_cell = VanillaRNNCell(self._hidden_size, bias=self._bias)\n",
    "        self.hidden_to_out = nn.Linear(self._hidden_size, self._output_size, bias=self._bias)\n",
    "    \n",
    "    def step(self, input, hidden=None):\n",
    "        input_ = self.in_to_hidden(input)\n",
    "        _, hidden_ = self.rnn_cell(input_, hidden=hidden)\n",
    "        output_ = self.hidden_to_out(hidden_)\n",
    "        \n",
    "        return output_, hidden_\n",
    "    \n",
    "    def forward(self, inputs, hidden=None, force=True, warm_start=10):\n",
    "        steps = len(inputs)\n",
    "        \n",
    "        outputs = torch.autograd.Variable(torch.zeros(steps, self._output_size, self._output_size))\n",
    "        \n",
    "        output_ = None\n",
    "        hidden_ = hidden\n",
    "        \n",
    "        for i in range(steps):\n",
    "            if force or i == 0:\n",
    "                input_ = inputs[i]\n",
    "            else:\n",
    "                if i < warm_start:\n",
    "                    input_ = inputs[i]\n",
    "                else:\n",
    "                    input_ = output_\n",
    "                \n",
    "            output_, hidden_ = self.step(input_, hidden_)\n",
    "            outputs[i] = output_\n",
    "            \n",
    "        return outputs, hidden_\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train RNN on sine wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f'Running code @ {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNROLL_LENGTH = 30  #@param {type:\"integer\"}\n",
    "NUM_ITERATIONS = 10000  #@param {type:\"integer\"}\n",
    "WARM_START = 10  #@param {type:\"integer\"}\n",
    "TEACHER_FORCING = False  #@param {type:\"boolean\"}\n",
    "HIDDEN_UNITS = 20  #@param {type:\"integer\"}\n",
    "LEARNING_RATE = 0.0001  #@param {type:\"number\"}\n",
    "REPORTING_INTERVAL = 200  #@param {type:\"integer\"}\n",
    "\n",
    "# We create training data, sine wave over [0, 2pi]\n",
    "x_train = np.arange(0, 2*np.pi, 0.1).reshape(-1, 1, 1)\n",
    "y_train = np.sin(x_train)\n",
    "\n",
    "net = VanillaRNN(hidden_size=HIDDEN_UNITS, bias=False)\n",
    "net.train()\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "running_loss = 0\n",
    "\n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "    # select a start point in the training set for a sequence of UNROLL_LENGTH\n",
    "    start = np.random.choice(range(x_train.shape[0] - UNROLL_LENGTH))\n",
    "    train_sequence = y_train[start : (start + UNROLL_LENGTH)]\n",
    "    \n",
    "    train_inputs = torch.from_numpy(train_sequence[:-1]).float().to(device)\n",
    "    train_targets = torch.from_numpy(train_sequence[1:]).float().to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs, hidden = net(train_inputs, hidden=None, force=TEACHER_FORCING, warm_start=WARM_START)\n",
    "    loss = criterion(outputs, train_targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if iteration % REPORTING_INTERVAL == REPORTING_INTERVAL - 1:\n",
    "        # let's see how well we do on predictions for the whole sequence\n",
    "        avg_loss = running_loss / REPORTING_INTERVAL\n",
    "        \n",
    "        report_sequence = torch.from_numpy(y_train[:-1]).float().to(device)\n",
    "        report_targets = torch.from_numpy(y_train[1:]).float().to(device)\n",
    "        report_output, report_hidden = net(report_sequence, hidden=None, force=False, warm_start=WARM_START)\n",
    "        \n",
    "        report_loss = criterion(report_output, report_targets)\n",
    "        print('[%d] avg_loss: %.5f, report_loss: %.5f, ' % (iteration + 1, avg_loss, report_loss.item()))\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.title('Training Loss %.5f;  Sampling loss %.5f; Iteration %d' % (avg_loss, report_loss.item(), iteration))\n",
    "        \n",
    "        plt.plot(y_train[1:].ravel(), c='blue', label='Ground truth',\n",
    "               linestyle=\":\", lw=6)\n",
    "        plt.plot(range(start, start+UNROLL_LENGTH-1), outputs.data.numpy().ravel(), c='gold',\n",
    "               label='Train prediction', lw=5, marker=\"o\", markersize=5,\n",
    "               alpha=0.7)\n",
    "        plt.plot(report_output.data.numpy().ravel(), c='r', label='Generated', lw=4, alpha=0.7)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-jCR9YGaI7my"
   },
   "source": [
    "### Train the RNN\n",
    "\n",
    "Train the RNN on sine data - predict the next sine value from *predicted* sine values.\n",
    "\n",
    "Predict   $$ sin (x +t \\epsilon) $$ from $$ sin (x), sin (x + \\epsilon), ..., sin (x + (t-1) \\epsilon) $$\n",
    "\n",
    "In particular, we want the network to predict the next value in a loop, conditioning the prediction on some initial values (provided) and all subsequent predictions.\n",
    "\n",
    "To learn the prediction model, we will use *teacher forcing*. This means that when training the model, the input at time $t$ is the real sequence at time $t$, rather than the output produced by the model at $t-1$. When we want to generate data from the model, we do not have access to the true sequence, so we do not use teacher forcing. However, in the case of our problem, we will also use *warm starting*, because we require multiple time steps to predict the next sine wave value (at least 2, for the initial value and for the step). \n",
    "\n",
    "The code below unrolls the RNN core you have defined above, does the training using backprop though time and plots the real data (\"ground truth\"), the data generated during training (\"train predictions\") and the model samples \"generated\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gXXQSfxREqKf"
   },
   "source": [
    "Please add your final sampling losses to this spreadsheet in a new row: https://docs.google.com/spreadsheets/d/1Zpi_A6RP89E00vurqz9dRHYCd29PqzB9VB7Y4giydyA/edit#gid=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gOKWVWE9sDke"
   },
   "outputs": [],
   "source": [
    "# Default hypers:\n",
    "# UNROLL_LENGTH = 30  #@param {type:\"integer\"}\n",
    "# NUM_ITERATIONS = 10000  #@param {type:\"integer\"}\n",
    "# WARM_START = 2  #@param {type:\"integer\"}\n",
    "# TEACHER_FORCING = False  #@param {type:\"boolean\"}\n",
    "# HIDDEN_UNITS = 20  #@param {type:\"integer\"}\n",
    "# LEARNING_RATE = 0.0001  #@param {type:\"number\"}\n",
    "# REPORTING_INTERVAL = 2000  #@param {type:\"integer\"}\n",
    "\n",
    "# You may want to try:\n",
    "# default hypers with/without teacher forcing\n",
    "# use UNROLL_LENGTH = 62 to train on the whole sequence (is teacher forcing useful?)\n",
    "# use UNROLL_LENGTH = 62, no teacher forcing and warm_start = 2 # this should break training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLuIAK8LJWay"
   },
   "source": [
    "**Note:** initialization is not fixed (we do not fix a random seed), so each time the cell is executed, the parameters take new initial values and hence training can lead to different results. What happens if you run it multiple times?\n",
    "\n",
    "###What is worth trying/understanding here?\n",
    "\n",
    "* Difference between teacher forcing and learning on own samples:\n",
    " * What are the pros and cons of teacher forcing?\n",
    " * Why is the model struggling to learn in one of the setups?\n",
    " * What is it we actually care about for models like this? What should be the actual surrogate?\n",
    "* How does warm starting affect our training? Why?\n",
    "* What happens if the structure of interest is much longer than the unroll length?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g7Bj03cut3Ex"
   },
   "source": [
    "Answers:\n",
    "* Teacher forcing because BPTT is much easier and works better in practice. Intuition is similar to immitation learning. Without TF it is very hard to learn because error tend to accumulate. If you use TF then you get very local structure.\n",
    "* No teacher forcing makes training very difficult.\n",
    "* Depending on what you want to model, this loss may be fine if you care about probabilities but not generating samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uxyUegmC5_Hj"
   },
   "source": [
    "# Ex. 2      Vanishing and exploding gradients\n",
    "\n",
    "Given an input sequence $(x_1, ..., x_N)$ of random floats (sampled from normal distribution), train an RNN as before and compute the gradients of the last output state w.r.t. every previous state:\n",
    "$$\n",
    "\\left \\| \\frac{\\partial h_{N}}{\\partial h_i} \\right \\|\n",
    "$$\n",
    "for each unroll $i$, and plot these quantities for various RNNs.\n",
    "\n",
    "Note, that during learning one would compute\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta}  \n",
    "$$\n",
    "which, using chain rule will involve terms like\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_N} \\cdot\n",
    "\\frac{\\partial h_N}{\\partial h_{N-1}} \\cdot\n",
    "\\dots \\cdot\n",
    "\\frac{\\partial h_i}{\\partial h_{i-1}} \\cdot\n",
    "\\dots \\cdot\n",
    "\\frac{\\partial h_0}{\\partial \\theta}\n",
    "$$\n",
    "so if one of them vanishes, all of them do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ULSWaWPtpynM"
   },
   "source": [
    "# Hints:\n",
    "\n",
    "Tensorflow already defines many types of RNN Cells, such as LSTM, GRU, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAL8UB3QvoF7"
   },
   "source": [
    "NB: There is no training here, we are just computing the norms of the gradients of the last hidden state with respect to the hidden state across steps in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "zljqN01vc9-3"
   },
   "outputs": [],
   "source": [
    "#@title Vanishing and exploding gradients\n",
    "\n",
    "SEQ_LENGTH = 15  #@param {type:\"integer\"}\n",
    "HIDDEN_UNITS = 20  #@param {type:\"integer\"}\n",
    "\n",
    "dummy_input = [torch.from_numpy(np.array([[np.random.normal()]])) for _ in range(SEQ_LENGTH)] \n",
    "\n",
    "######################\n",
    "#   YOUR CODE HERE   #\n",
    "######################\n",
    "# Add several cell constructors (use those already defined in Tensorflow) to the\n",
    "# list (e.g., also add a GRU, and a few more LSTMS with their initial \n",
    "# forget_bias values set to: 0, +1, +2 and -2).\n",
    "# If in doubt, check the documentation.\n",
    "\n",
    "def _set_forget_bias(lstm_cell, fill_value=0.):\n",
    "    # The bias terms in the lstm_cell are arranged as bias_input_gate, bias_forget_gate, bias_gain_gate, bias_output_gate\n",
    "    # To alter the forget_gate bias, we need to modify the parameters from 1/4 to 1/2 of the length of the bias vectors\n",
    "    for name, _ in lstm_cell.named_parameters():\n",
    "        if \"bias\" in name:\n",
    "            bias = getattr(lstm_cell, name)\n",
    "            n = bias.size(0)\n",
    "            start, end = n//4, n//2\n",
    "            bias.data[start:end].fill_(float(fill_value))\n",
    "            \n",
    "    return lstm_cell\n",
    "\n",
    "\n",
    "### Solution\n",
    "rnn_types = {\n",
    "    'LSTM (0)': lambda nhid:  _set_forget_bias(nn.modules.LSTMCell(input_size=1, hidden_size=nhid), fill_value=0.),\n",
    "    'LSTM (+1)': lambda nhid:  _set_forget_bias(nn.modules.LSTMCell(input_size=1, hidden_size=nhid), fill_value=1.),\n",
    "    'LSTM (-2)': lambda nhid:  _set_forget_bias(nn.modules.LSTMCell(input_size=1, hidden_size=nhid), fill_value=-2.),\n",
    "    'LSTM (+2)': lambda nhid:  _set_forget_bias(nn.modules.LSTMCell(input_size=1, hidden_size=nhid), fill_value=2.),\n",
    "    'LSTM (+10)': lambda nhid:  _set_forget_bias(nn.modules.LSTMCell(input_size=1, hidden_size=nhid), fill_value=10.),\n",
    "    'GRU': lambda nhid: nn.modules.GRUCell(input_size=1, hidden_size=nhid),\n",
    "    'RNN': lambda nhid: VanillaRNN(input_size=1, hidden_size=nhid),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "depths = {rnn_type: [] for rnn_type in rnn_types}\n",
    "grad_norms = {rnn_type: [] for rnn_type in rnn_types}\n",
    "\n",
    "for rnn_type in rnn_types:\n",
    "  \n",
    "    constructor = rnn_types[rnn_type]\n",
    "    rnn = constructor(HIDDEN_UNITS)\n",
    "    \n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    rnn_at_time = []\n",
    "    gradients_at_time = []\n",
    "    \n",
    "    prev_state = None\n",
    "    \n",
    "    for i in range(SEQ_LENGTH):\n",
    "        if prev_state is None:\n",
    "            prev_state = rnn(dummy_input[i].float())\n",
    "        else:\n",
    "            if rnn_type.startswith('RNN'):\n",
    "                prev_state = rnn(dummy_input[i].float(), hidden=prev_state[1])\n",
    "            else:\n",
    "                prev_state = rnn(dummy_input[i].float(), prev_state)\n",
    "        \n",
    "        if rnn_type.startswith('LSTM'):\n",
    "            prev_state[0].retain_grad()\n",
    "            prev_state[1].retain_grad()\n",
    "            rnn_at_time.append(prev_state[1])\n",
    "            \n",
    "        elif rnn_type.startswith('GRU'):\n",
    "            prev_state.retain_grad()\n",
    "            rnn_at_time.append(prev_state)\n",
    "        \n",
    "        elif rnn_type.startswith('RNN'):\n",
    "            prev_state[1].retain_grad()\n",
    "            rnn_at_time.append(prev_state[1])\n",
    "    \n",
    "    # We don't really care about the loss here: we are not solving a specific \n",
    "    # problem, any loss will work to inspect the behavior of the gradient.\n",
    "    dummy_loss = torch.sum(rnn_at_time[-1])\n",
    "    dummy_loss.backward()\n",
    "    \n",
    "    for i in range(1, SEQ_LENGTH):\n",
    "        current_gradient = rnn_at_time[i].grad\n",
    "        gradients_at_time.append(current_gradient)\n",
    "    \n",
    "    for gid, grad in enumerate(gradients_at_time):\n",
    "        depths[rnn_type].append(len(gradients_at_time) - gid)    \n",
    "        grad_norms[rnn_type].append(np.linalg.norm(grad))\n",
    "        \n",
    "    dummy_loss.detach_()\n",
    "\n",
    "plt.figure()\n",
    "for rnn_type in depths:\n",
    "    plt.plot(depths[rnn_type], grad_norms[rnn_type], label=\"%s\" % rnn_type, alpha=0.7, lw=2)\n",
    "plt.legend()  \n",
    "plt.ylabel(\"$ \\\\| \\\\partial \\\\sum_i {c_{N}}_i / \\\\partial c_t \\\\|$\", fontsize=15)\n",
    "plt.xlabel(\"Steps through time - $t$\", fontsize=15)\n",
    "plt.xlim((1, SEQ_LENGTH-1))\n",
    "plt.title(\"Gradient magnitudes across time for: RNN-Type (forget_bias value)\")\n",
    "#plt.savefig(\"mygraph.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-4v7TUtjKHD-"
   },
   "source": [
    "### What do we learn from this?\n",
    "\n",
    "This particular experiment is an extremely simple surrogate for actual problem, but shows a few interesting aspects:\n",
    "\n",
    "* Is LSTM by construction free of *exploding* gradients too?\n",
    "* What are other ways of avoiding explosions you can think of?\n",
    "* Does initialisation (of gates here, but in general) matter a lot?\n",
    "* Does this look like a solution that can really scale time-wise? Say to be doing credit assignment through years of experience?\n",
    "* If not, what might be a next step?\n",
    "\n",
    "See http://proceedings.mlr.press/v37/jozefowicz15.pdf for a more detailed discussion of the effect of the forget gate bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HfEVZV3WwNEc"
   },
   "source": [
    "Canonical Answers:\n",
    "* If you make forget_bias=10 the the gradients will 'explode'\n",
    "* No. LSTM still has the problem but it can be a little less bad. Clipping is very often used by default to introduce some robustness.\n",
    "* Gradient clipping.\n",
    "* Init matters. Identity hidden_to_hidden can help alleviate gradient 'explosion'.\n",
    "* No scalable to train with very long sequences.\n",
    "* Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nf74B2tbcnYh"
   },
   "source": [
    "# Ex. 3    Language Modelling\n",
    "\n",
    "Now we will train a character level RNN on text data - specifically Shakespeare sonnets. We will reuse the same concepts, such as teacher forcing and different types of RNN cores. \n",
    "\n",
    "At the end of the exercise, after you have filled in the TextModel class, you can train the model and see that in generates text that has sonnet structure and learns words.  You should focus on the TextModel class implementation, and leverage the code provided to do the training and visualization and data loading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_uGGyiZyI3w"
   },
   "source": [
    "## Ex 3.1   Analysis of single neurons and gates\n",
    "\n",
    "We will now look at the individual activations of neurons in a Recurrent network. For this to work, you need to have completed the previous exercise in which you expose the network activations, as well as train a model.\n",
    "\n",
    "For a similar analysis, see [this paper](https://arxiv.org/pdf/1506.02078.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "5gU06mESEcoT"
   },
   "outputs": [],
   "source": [
    "#@title String plot function\n",
    "\n",
    "def string_plot(chars, values, title=None):\n",
    "  \"\"\"\n",
    "  Given a string \"chars\" and a vector of numbers \"values\" of the same length\n",
    "  displays the string, using \"|\" as EOL symbol, and colors each character\n",
    "  background using corresponding value in values\n",
    "  \"\"\"\n",
    "  \n",
    "  assert len(chars) == len(values)\n",
    "  \n",
    "  lines = []\n",
    "  line = \"\"\n",
    "  for char in chars:\n",
    "    if char != '|':\n",
    "      line += char\n",
    "    else:\n",
    "      line += \" \"\n",
    "      lines.append(line)\n",
    "      line = \"\"\n",
    "  lines.append(line)\n",
    "  \n",
    "  height = len(lines) \n",
    "  width = max(map(len, lines))\n",
    "    \n",
    "  data = np.zeros((height, width))\n",
    "  data[:,:] = np.nan\n",
    "  \n",
    "  pos = 0\n",
    "  for lid, line in enumerate(lines):\n",
    "    data[lid, :len(line)] = values[pos: pos+len(line)]\n",
    "    pos = pos+len(line)\n",
    "    \n",
    "  assert pos == len(values)\n",
    "    \n",
    "  plt.figure(figsize=(width * 0.3, height * 0.3))\n",
    "  plt.title(title)\n",
    "  plt.imshow(data.reshape((height, width)), interpolation='none',\n",
    "             cmap='Reds', alpha=0.5)\n",
    "  plt.axis('off')\n",
    "  \n",
    "  for lid, line in enumerate(lines):\n",
    "    for cid, char in enumerate(line):\n",
    "      plt.text(cid-0.2,lid+0.2,char,color='k',fontsize=9)\n",
    "\n",
    "  plt.show()\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lTOoND9nNGyp"
   },
   "source": [
    "### What kind of neurons can we expect to find?\n",
    "\n",
    "* Lots of counting neurons (their activity is just growing/decreasing independently from input)\n",
    "* Names neuron - activates around names of people in the play, such as HAMLET: or JOHN OF GAUNT:\n",
    "* Line width neuron - with activity proportional to the length of the current line (number of charaters since last \"|\")\n",
    "* Paragraph length neuron - activity proportional to the length of the paragraph in lines\n",
    "* Special character neurons - such as coding for probability of generating \":\"\n",
    "* Many, many mixtures of the above\n",
    "\n",
    "Note, that if neurons like these do not appear it does not mean that network does not \"know\" these elements. Highly discriminative, single neuron decoupling is not something neural networks are trained to do, it is just an empirical observation, shared across many domains (cat neurons in visual classifiers etc.). Knowledge can be represented in many other ways, in particular the fact that it is represented in a single neuron does not mean network does not have a distributed \"backup\" of the same knowledge somewhere else.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtOHP2haXQ9F"
   },
   "source": [
    "## Ex 3.2   Analysis of the state dynamics\n",
    "\n",
    "In this exercise, we will visualize the activations in a different way, by projecting them to 2 dimensions, via dimensionality reduction. \n",
    "\n",
    "When using different projection techniques, you willl see different results. For example, PCA will display the directions with most variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oZmBGOnXxRsI"
   },
   "source": [
    "### So what am I looking at?\n",
    "\n",
    "2D projections of high-dimensional spaces are always loosing a lot of information, however the general structure can still be recovered. Here, one can see that both paragraph-splits and line-splits can be decoded by just looking at the dynamics of the hidden state, giving more insights into internals of an RNN. Note, that contrary to single-neuron analysis, here we are truly looking at the whole picture, thus what is observed is likely behind the dynamics of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "46pynnpd5nx-"
   },
   "source": [
    "Canonical Answer:\n",
    "Recurrent nets use the dimensions of the hidden state to encode position in the sequence, like a counter. The trajectory through the hidden space can be thought of some form of memory. E.g. the RNN could be storing bits of information by setting a dimension to +1 or -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fpXpLbLVobgN"
   },
   "source": [
    "# Done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XwojS8kx0QyE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EEML2019_RNN_full.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
