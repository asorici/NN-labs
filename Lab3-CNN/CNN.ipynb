{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ComputerVisionPart1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xi1dtiTtK1Ov"
      },
      "source": [
        "Lab 3 : Convolutional Neural Networks\n",
        "=====================================\n",
        "\n",
        "### Supervised classification, overfitting and inductive biases in convnets\n",
        "### Credit to EEML 2019 ConvNet and Computer Vision tutorial by Viorica Patraucean \n",
        "### Credit to PyTorch model-zoo for ResNet-50 implementation\n",
        "\n",
        "* Exercise 1: Write PyTorch data Transformers to augment/alter training and testing data with random flips, random crops and random pixel permutations\n",
        "* Exercise 2: Implement and train a Resnet-50 classifier using supervised learning; enable/disable batch norm updates to see the effect.\n",
        "* Exercise 3: Inductive biases in convnets; comparison with MLP.\n",
        "* Exercise 4: Overfitting and regularization using weight decay.\n",
        "\n",
        "**Questions**: \n",
        "1. What happens with resnet's performance when batch norm statistics are not updated? How about MLP? Why is one affected less than the other?\n",
        "*A: If the batch statistics are not updated, resnet's performance is similar to a random classifier. The MLP performs considerably better than chance, due to its shallow depth.*\n",
        "2. What is resnet's train loss on permuted cifar? How about the test accuracy? How is the MLP affected by the permutation?\n",
        "*A: Resnet fits perfectly the training set (100% train accuracy), but generalises very poorly (around 45%). It manages to memorise the training set due to its high capacity and small dataset, but cannot generalise. Note that this would not happen on Imagenet, i.e. resnet would not be able to memorize the dataset. The performance of a shallow 2-layer MLP is much better than resnet on permuted cifar (65%). Due to fully connectedness, the MLP is not affected by the permutation. The same would happen for a Transformer style model. This shows the strong effect that the inductive biases (here mainly locality of the data) have on the generalisation power of convnets.*  \n",
        "3. What other types of regularization could you use to avoid overfitting?\n",
        "*A: dropout*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Ho_PioVRL_o",
        "outputId": "3e817e11-6142-44e0-98e9-c1f9777e016b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f'Running code @ {device}')\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Plotting library.\n",
        "from matplotlib import pyplot as plt\n",
        "import pylab as pl\n",
        "from IPython import display\n",
        "\n",
        "import collections\n",
        "import enum\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running code @ cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmjogSWJ-Xlt",
        "colab_type": "text"
      },
      "source": [
        "## Download and prepare the data\n",
        "\n",
        "* Cifar-10 equivalent of MNIST for natural RGB images\n",
        "\n",
        "* 60000 32x32 colour images in 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n",
        "\n",
        "* train: 50000; test: 10000\n",
        "\n",
        "\n",
        "**TODO Task 1: Implement data augmentation**\n",
        "\n",
        "In Pytorch one can perform data transformations when loading a dataset using predefined (from `torchvision.transforms`) or custom build _transform_ classes. \n",
        "In this exercise you are to augment the _training time_ preprocessing with two data augmentation techniques: random image flips and random crops, which are found in the `torchvision.transforms` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LEEkhc5KRa8t",
        "outputId": "cf7c5d63-c851-4cc3-b00a-dacc790ffc70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Prepare data \n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "\n",
        "TRAIN_DATASET_SIZE = 50000\n",
        "TEST_DATASET_SIZE = 10000\n",
        "\n",
        "CIFAR10_IMG_WIDTH = 32\n",
        "CIFAR10_IMG_HEIGHT = 32\n",
        "\n",
        "DATA_MEAN = (0.5, 0.5, 0.5)\t\t# define the mean for the scaling transform - PIL images already come given in \n",
        "DATA_STD = (0.5, 0.5, 0.5)\t\t# define the standard deviation for the scaling transform\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomHorizontalFlip(),              # TODO: apply random horizontal flip\n",
        "        transforms.RandomCrop(                          # TODO: apply random crop, after padding image with 4 values on each side, using `reflect` mode\n",
        "            size=(CIFAR10_IMG_WIDTH, CIFAR10_IMG_HEIGHT), \n",
        "            padding=(4, 4), \n",
        "            padding_mode=\"reflect\"),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(DATA_MEAN, DATA_STD)       # normalize the image tensor to [-1, 1] on each channel: img_norm = (img - data_mean) / data_std \n",
        "    ] \n",
        ")\n",
        "\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),                          # on test set we only need to apply the same normalization\n",
        "        transforms.Normalize(DATA_MEAN, DATA_STD) \n",
        "    ] \n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "train_images = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, \n",
        "                                            transform=train_transform)\n",
        "\n",
        "test_images = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, \n",
        "                                           transform=test_transform)\n",
        "\n",
        "# Check sizes of tensors\n",
        "print(f'Size of training images {train_images.data.shape}')\n",
        "print(f'Size of training labels {len(train_images.targets)}')\n",
        "print(f'Size of test images {test_images.data.shape}')\n",
        "print(f'Size of test labels {len(test_images.targets)}')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 99%|█████████▉| 168435712/170498071 [00:12<00:00, 17291651.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Size of training images (50000, 32, 32, 3)\n",
            "Size of training labels 50000\n",
            "Size of test images (10000, 32, 32, 3)\n",
            "Size of test labels 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4vMPjp0UU4Mx"
      },
      "source": [
        "## Display the images\n",
        "The gallery function below shows sample images from the data, together with their labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xy0BWFwFUQ0J",
        "colab": {}
      },
      "source": [
        "MAX_IMAGES = 10\n",
        "\n",
        "\n",
        "def gallery(images, label, title='Input images'):\n",
        "    class_dict = ['plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "    num_frames, h, w, num_channels = images.shape\n",
        "    num_frames = min(num_frames, MAX_IMAGES)\n",
        "    ff, axes = plt.subplots(1, num_frames, figsize=(num_frames, 1), subplot_kw={'xticks': [], \n",
        "                                                                                'yticks': []})\n",
        "    for i in range(0, num_frames):\n",
        "        if num_channels == 3:\n",
        "            axes[i].imshow(np.squeeze(images[i]))\n",
        "        else:\n",
        "            axes[i].imshow(np.squeeze(images[i]), cmap='gray')\n",
        "        axes[i].set_title(class_dict[label[i]])\n",
        "        plt.setp(axes[i].get_xticklabels(), visible=False)\n",
        "        plt.setp(axes[i].get_yticklabels(), visible=False)\n",
        "    ff.subplots_adjust(wspace=0.1)\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWvJh11N-rYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7kGaRa23RfjT",
        "outputId": "93c17195-d219-4398-9859-689594a9a604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "gallery(train_images.data, train_images.targets)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-23702e999fff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgallery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'gallery' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9pgKO2uEU_tn"
      },
      "source": [
        "## Prepare the data for training and testing\n",
        "* for training, we use stochastic optimizers (e.g. SGD, Adam), so we need to sample at random mini-batches from the training dataset\n",
        "* for testing, we iterate sequentially through the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a7DbXWyoRjO6",
        "outputId": "c41ff867-0428-415b-dbc0-e13c7d7abe54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# define dimension of the batches to sample from the datasets\n",
        "BATCH_SIZE_TRAIN = 100  #@param\n",
        "BATCH_SIZE_TEST = 100  #@param\n",
        "NO_WORKERS = 8  #@param\n",
        "SHUFFLE_DATA = True\n",
        "\n",
        "# create Dataset iterator object using the data previously downloaded\n",
        "# we shuffle the data and sample repeatedly batches for training\n",
        "train_loader = torch.utils.data.DataLoader(train_images, batch_size=BATCH_SIZE_TRAIN, \n",
        "                                           shuffle=SHUFFLE_DATA, \n",
        "                                           num_workers=NO_WORKERS)\n",
        "\n",
        "# get a training batch of images and labels\n",
        "(batch_train_images, batch_train_labels) = next(iter(train_loader))\n",
        "\n",
        "# check that the shape of the training batches is the expected one\n",
        "print(f'Shape of training images: {batch_train_images.size()}')\n",
        "print(f'Shape of training labels: {batch_train_labels.size()}')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of training images: torch.Size([100, 3, 32, 32])\n",
            "Shape of training labels: torch.Size([100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gpHYjHEiRmvs",
        "outputId": "39b420e6-e76b-4383-f599-fddf7da0e985",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# we do the same for test dataset\n",
        "test_loader = torch.utils.data.DataLoader(test_images, batch_size=BATCH_SIZE_TRAIN, \n",
        "                                          shuffle=SHUFFLE_DATA, \n",
        "                                          num_workers=NO_WORKERS)\n",
        "\n",
        "def loopy_test_loader(dl):\n",
        "    data_iter = iter(dl)\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            yield next(data_iter)\n",
        "        except StopIteration:\n",
        "            data_iter = iter(dl)\n",
        "            yield next(data_iter)\n",
        "\n",
        "(batch_test_images, batch_test_labels) = next(iter(test_loader))\n",
        "print(f'Shape of test images: {batch_test_images.size()}')\n",
        "print(f'Shape of test labels: {batch_test_labels.size()}')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of test images: torch.Size([100, 3, 32, 32])\n",
            "Shape of test labels: torch.Size([100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Q9xIZOJEiiU"
      },
      "source": [
        "## General setting; use the options below to switch between exercises."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6b8dDvmpEgwM",
        "colab": {}
      },
      "source": [
        "model = \"mlp\"  # @param['resnet_v2','mlp']\n",
        "flag_batch_norm = 'ON'  # @param['ON', 'OFF']\n",
        "flag_permute = True  # @param['True', 'False'] {type:\"raw\"}\n",
        "flag_regularize = True  # @param['True', 'False'] {type:\"raw\"}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F9W8ggEBVlcG"
      },
      "source": [
        "## Preprocess input for training and testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJBFe8d6-XmG",
        "colab_type": "text"
      },
      "source": [
        "### Random pixel permutation transform\n",
        "\n",
        "The random pixel permutation transform is used for **Task 2**, analysing the spatial locality inductive bias of CNNs. \n",
        "You will have to add this type of transform to the end of the list of data transforms, if `flag_permute` is set to `True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "RDTKjPO_4GLG",
        "colab": {}
      },
      "source": [
        "class RandomPermute(object):\n",
        "    \"\"\"\n",
        "    An implementation of a custom PyTorch data transformation class.\n",
        "    The transform will apply a constant random pixel permutation of the image tensor.\n",
        "    The permutation is applied to a subset of pixels up to a factor `f` in [0, 1].\n",
        "    The default is to permute all pixels.\n",
        "    \"\"\"\n",
        "    def __init__(self, width: int = 32, height: int = 32, factor: float = 1.):\n",
        "        # The constructor of the transformer defines the random subset of pixels to be permuted\n",
        "        # and the random permutation order of these pixels\n",
        "        \n",
        "        # define an initial seed for the random generator to obtain the same random\n",
        "        # permutation for each image\n",
        "        rnd_generator = np.random.RandomState(10)\n",
        "        pixel_coord = np.arange(0, width * height)\n",
        "        rnd_generator.shuffle(pixel_coord)\n",
        "        \n",
        "        # Select pixel coord to permute\n",
        "        pixel_coord = pixel_coord[:int(factor * len(pixel_coord))]  \n",
        "        \n",
        "        # Define permute order\n",
        "        permute_order = np.arange(0, len(pixel_coord))  \n",
        "        rnd_generator.shuffle(permute_order)\n",
        "        \n",
        "        self.pixel_coord = pixel_coord\n",
        "        self.permute_order = permute_order\n",
        "\n",
        "    def __call__(self, image):\n",
        "        # The __call__ is made for each image tensor \n",
        "        pixel_coord = self.pixel_coord\n",
        "        permute_order = self.permute_order\n",
        "\n",
        "        image_size = image.size()\n",
        "        image = image.view(image.size(0), -1)\n",
        "        # Permute pixels\n",
        "        image[:, pixel_coord] = image[:, pixel_coord][:, permute_order]\n",
        "        image = image.view(image_size)\n",
        "        \n",
        "        return image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA8hnT4H-XmK",
        "colab_type": "text"
      },
      "source": [
        "## Defining RESNET-50 Blocks\n",
        "\n",
        "The PyTorch implementation of ResNet-50 defines 4 logical \"layers\" of _ResNet block_ groups, which are set up according to the number of channels that result after each logical layer.\n",
        "\n",
        "The configuration below specifies that there the network is composed of:\n",
        "  * 3 ResNet blocks with 64 channels and a _\"first-block-dimension-reduction-stride\"_ of 1\n",
        "  * 4 ResNet blocks with 128 channels and a _\"first-block-dimension-reduction-stride\"_ of 2 (halfing the input size)\n",
        "  * 6 ResNet blocks with 256 channels and a _\"first-block-dimension-reduction-stride\"_ of 2 (halfing the input size)\n",
        "  * 3 ResNet blocks with 512 channels and a _\"first-block-dimension-reduction-stride\"_ of 2 (halfing the input size)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fRsh0ZUeV6m2",
        "colab": {}
      },
      "source": [
        "# define parameters of resnet blocks for resnet-50 model\n",
        "import collections\n",
        "\n",
        "ResNetBlockParams = collections.namedtuple(\n",
        "    \"ResNetBlockParams\", [\"neck_ch\", \"blocks\", \"stride\"])\n",
        "\n",
        "DOWNSIZE_FACTOR = 4     # @param[8, 4, 2, 1]\n",
        "\n",
        "BLOCKS_50 = (\n",
        "    ResNetBlockParams(int(64/DOWNSIZE_FACTOR), 3, 1),\n",
        "    ResNetBlockParams(int(128/DOWNSIZE_FACTOR), 4, 2),\n",
        "    ResNetBlockParams(int(256/DOWNSIZE_FACTOR), 6, 2),\n",
        "    ResNetBlockParams(int(512/DOWNSIZE_FACTOR), 3, 2)\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEHrKw71-XmO",
        "colab_type": "text"
      },
      "source": [
        "### TODO 1.1: define the convolution operations inside the ResNet-50 block\n",
        "You have to define the 3x3 and 1x1 convolutions that compose the ResNet-50 block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cMC4BWe-XmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1, padding=1):\n",
        "    \"\"\"\n",
        "    3x3 2D convolution with padding=1\n",
        "    @:param in_planes: the number of input channels for the convolution filter\n",
        "    @:param out_planes: the number of output channels for the convolution filter\n",
        "    @:param stride: stride value for the convolution filter, default is 1\n",
        "    @:param padding: the padding to apply, default is 1 to keep width and height the same in the output activation maps \n",
        "    \"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=padding, bias=False)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"\n",
        "    1x1 2D convolution\n",
        "    @:param in_planes: the number of input channels for the convolution filter\n",
        "    @:param out_planes: the number of output channels for the convolution filter\n",
        "    @:param stride: stride value for the convolution filter, default is 1\n",
        "    \"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPF33y9U-XmS",
        "colab_type": "text"
      },
      "source": [
        "### TODO 1.2 Define the ResNet block with \"bottleneck\"\n",
        "\n",
        "The ResNet-50 _Bottleneck_ block is defined as in the following figure.\n",
        "\n",
        "![ResNet-50 Bottleneck block](https://github.com/asorici/NN-labs/blob/master/Lab3-CNN/img/Bottleneck-Blocks-for-ResNet-50-left-identity-shortcut-right-projection-shortcut.png?raw=1)\n",
        "\n",
        "The image on the left displays the case where `stride=1` and the block performs no _downsampling_ of the input because the conv 3x3 on the residual part keeps the same dimensions (using stride=1).\n",
        "\n",
        "The image on the right shows the operations when `stride=2` and the output of the bottleneck layer is one where _downsampling_ needs to be performed on the input to match the halfing of the image size on the residual path.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xby5GIFGaIEh",
        "colab": {}
      },
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None,\n",
        "                 base_width=int(64/DOWNSIZE_FACTOR), norm_layer=nn.BatchNorm2d):\n",
        "        \n",
        "        super(Bottleneck, self).__init__()\n",
        "        \n",
        "        # `inplanes` defines the initial number of input channels for the bottleneck block\n",
        "        # `width` encodes the number of filters `f` in the picture of the bottleneck block above \n",
        "        width = int(planes * (base_width / (64./DOWNSIZE_FACTOR)))\n",
        "        \n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        \n",
        "        # first conv 1x1 layer which implements the bottlenecking\n",
        "        self.conv1 = conv1x1(inplanes, width) \n",
        "        self.bn1 = norm_layer(width)\n",
        "        \n",
        "        # conv 3x3 layer which applies the stride given as parameter\n",
        "        self.conv2 = conv3x3(width, width, stride) \n",
        "        self.bn2 = norm_layer(width)\n",
        "        \n",
        "        # conv 1x1 layer where the number of output channels is \"expanded\" according to the defined expansion factor\n",
        "        # according to the above figure, the expansion factor is set to 4\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        \n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        \n",
        "        # first layer conv1 + bn1 + relu\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        \n",
        "        # second layer conv2 + bn2 + relu\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        \n",
        "        # third layer conv3 + bn3\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        \n",
        "        # apply downsample to identity (original input to block), if it is defined (right image in figure above)\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "        \n",
        "        # add identity to residual and apply relu\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s14QG5Et-XmW",
        "colab_type": "text"
      },
      "source": [
        "### Defining the ResNet-50 model\n",
        "<div>\n",
        "\t<img title=\"ResNet-50 Architecture\" src=\"https://github.com/asorici/NN-labs/blob/master/Lab3-CNN/img/ResNet-50-arch.jpg?raw=1\" width=\"300\"/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nfiQxXS-XmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=10, base_width=int(64/DOWNSIZE_FACTOR), \n",
        "                 norm_layer=nn.BatchNorm2d):\n",
        "        \"\"\"\n",
        "        Build the ResNet network model - based on PyTorch Model Zoo implementation\n",
        "        \n",
        "        :param block: the Resnet Block model to be used: in our case the `Bottleneck` block\n",
        "        :param layers: the logical layer definition, such as the one in BLOCKS_50\n",
        "        :param num_classes: the number of classes for target labels\n",
        "        :param base_width: the initial number of layers\n",
        "        :param norm_layer: the type of normalization to apply - either BatchNorm or EmptyNorm, default is BatchNorm\n",
        "        \"\"\"\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.inplanes = int(64/DOWNSIZE_FACTOR)\n",
        "        self.dilation = 1\n",
        "        self._norm_layer = norm_layer\n",
        "        \n",
        "        self.base_width = base_width\n",
        "        \n",
        "        # ResNet starts out with an initial 7x7 convolution with stride 2 that halves the input size\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)  # image is now 16x16\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "        # We skip the maxpool layer because it would reduce the size too much starting from a 32 x 32 image \n",
        "        # self.maxpool = nn.MaxPool2d(kernel_size=1, stride=1, padding=1)\n",
        "        \n",
        "        # -- Build resnet logical layers\n",
        "        l = layers\n",
        "        \n",
        "        # logical layer1 - 3 bottleneck blocks with 64 filters and stride=1 \n",
        "        self.layer1 = self._make_layer(block, l[0].neck_ch, l[0].blocks, stride=l[0].stride)    # image is 16x16\n",
        "        \n",
        "        # logical layer2 - 4 bottleneck blocks with 128 filters and stride=2  (i.e. first halving)\n",
        "        self.layer2 = self._make_layer(block, l[1].neck_ch, l[1].blocks, stride=l[1].stride)    # image is 8x8\n",
        "        \n",
        "        # logical layer3 - 6 bottleneck blocks with 256 filters and stride=2  (i.e. second halving)\n",
        "        self.layer3 = self._make_layer(block, l[2].neck_ch, l[2].blocks, stride=l[2].stride)    # image is 4x4\n",
        "        \n",
        "        # logical layer4 - 3 bottleneck blocks with 512 filters and stride=2  (i.e. third halving)\n",
        "        self.layer4 = self._make_layer(block, l[3].neck_ch, l[3].blocks, stride=l[3].stride)    # image is 2x2\n",
        "        \n",
        "        # final ResNet layers - average pooling reduces size to 1x1 + fully connected layer of size 2048\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        \n",
        "        self.fc = nn.Linear(int(512/DOWNSIZE_FACTOR) * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = list()\n",
        "        \n",
        "        # first Bottleneck block in the logical layer is the one that does the downsampling \n",
        "        layers.append(block(self.inplanes, planes, \n",
        "                            stride=stride, downsample=downsample,\n",
        "                            base_width=self.base_width, \n",
        "                            norm_layer=norm_layer))\n",
        "        \n",
        "        # the rest operate on the the downsampled result, keeping the size (i.e. using default stride of 1)\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes,\n",
        "                                base_width=self.base_width,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cFrYIsi1aUEm",
        "colab": {}
      },
      "source": [
        "from torchvision.models.resnet import model_urls\n",
        "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "\n",
        "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet50(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-50 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet50', Bottleneck, BLOCKS_50, pretrained, progress,\n",
        "                   **kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7gcsY1Mjaf_s"
      },
      "source": [
        "## Define simple MLP baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SYekdfZBaiTy",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, num_classes=10, norm_layer=nn.BatchNorm1d):\n",
        "        super(MLP, self).__init__()\n",
        "        in_size = 3 * 32 * 32\n",
        "        self.fc1 = nn.Linear(in_size, int(1024/DOWNSIZE_FACTOR))\n",
        "        self.bc1 = norm_layer(int(1024/DOWNSIZE_FACTOR))\n",
        "        self.fc2 = nn.Linear(int(1024/DOWNSIZE_FACTOR), int(1024/DOWNSIZE_FACTOR))\n",
        "        self.bc2 = norm_layer(int(1024/DOWNSIZE_FACTOR))\n",
        "        self.fc3 = nn.Linear(int(1024/DOWNSIZE_FACTOR), num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = self.bc1(F.relu(self.fc1(x)))\n",
        "        x = self.bc2(F.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_JHI42C-Xmh",
        "colab_type": "text"
      },
      "source": [
        "## Define Empty Normalization Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCkBbX4H-Xmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmptyNorm(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(EmptyNorm, self).__init__()\n",
        "        self._modules = dict()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QPex0rz3auId"
      },
      "source": [
        "## Set up training pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SoTsOIRQSPV0",
        "colab": {}
      },
      "source": [
        "# First define the preprocessing ops for the train/test data\n",
        "crop_height = 32  # @param \n",
        "crop_width = 32  # @param\n",
        "NUM_CLASSES = 10  # @param\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "117ebPugarCO"
      },
      "source": [
        "### Initialize network & Get predictions from either MLP baseline or convnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pfHEpqp3DZbR",
        "colab": {},
        "outputId": "8fec2fbc-2c64-427f-9e30-e4de49f81c25"
      },
      "source": [
        "blocks = BLOCKS_50\n",
        "\n",
        "net = None  # type: nn.Module\n",
        "\n",
        "\n",
        "if model == 'mlp':\n",
        "    if flag_batch_norm == \"ON\":\n",
        "        net = MLP(num_classes=NUM_CLASSES)\n",
        "    else:\n",
        "        net = MLP(num_classes=NUM_CLASSES, norm_layer=EmptyNorm)\n",
        "        \n",
        "else:  # model is resnet_v2\n",
        "    if flag_batch_norm == \"ON\":\n",
        "        net = resnet50(num_classes=NUM_CLASSES)\n",
        "    else:\n",
        "        net = resnet50(num_classes=NUM_CLASSES, norm_layer=EmptyNorm)\n",
        "    \n",
        "if flag_permute:\n",
        "    permute_transform = RandomPermute()\n",
        "    train_images.transform.transforms.append(permute_transform)\n",
        "    test_images.transform.transforms.append(permute_transform)\n",
        "\n",
        "\n",
        "net.train()  # Default after init is train\n",
        "net = net.to(device)  # Move network to device\n",
        "\n",
        "print(list(net.modules())[0])\n",
        "\n",
        "# Let us test that we can propagate a batch through the defined networks\n",
        "select = 2\n",
        "inputs = batch_train_images.to(device)[:select]\n",
        "target = batch_train_labels[:select]\n",
        "\n",
        "output = net(inputs)\n",
        "_, predicted = torch.max(output, 1)\n",
        "\n",
        "print(output)\n",
        "print(predicted)\n",
        "print(target)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (fc1): Linear(in_features=3072, out_features=256, bias=True)\n",
            "  (bc1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (bc2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n",
            "tensor([[ 0.0717,  0.2758,  0.4764, -0.4043, -0.2691, -0.0501,  0.3661,  0.4440,\n",
            "         -0.3368, -0.1589],\n",
            "        [-0.0608, -0.2716, -0.5734,  0.3073,  0.3070,  0.0527, -0.4111, -0.3787,\n",
            "          0.4219,  0.2367]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([2, 8], device='cuda:0')\n",
            "tensor([7, 9])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5-F4W5niV1sm",
        "colab": {}
      },
      "source": [
        "# Get number of parameters in a model by iterating through the model parameters\n",
        "def get_num_params(model):\n",
        "    num_params = 0\n",
        "    for params in model.parameters():\n",
        "        num_params += params.shape.numel()\n",
        "        \n",
        "    return num_params\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l7VKCh9ySddK",
        "colab": {},
        "outputId": "211916d9-252e-423b-ea96-806996b49071"
      },
      "source": [
        "# Get number of parameters in the model. Verify that we have implemented models correctly\n",
        "print(\"Total number of parameters of models\")\n",
        "print(str(net.__class__), \": \", get_num_params(net))  \n",
        "\n",
        "# should be on the order of 23M for default ResNet and 4M for default MLP\n",
        "# if scaled down by 4: 1.4M for ResNet, 856K for MLP\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of parameters of models\n",
            "<class '__main__.MLP'> :  856074\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xXwpnrFWSmBg",
        "colab": {}
      },
      "source": [
        "def top_k_accuracy(k, target, output):\n",
        "    batch_size = target.size(0)\n",
        "    \n",
        "    _, pred = output.topk(k, 1, True, True)\n",
        "    \n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.to(device).view(1, -1).expand_as(pred))\n",
        "\n",
        "    correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "    correct_k.mul_(100.0 / batch_size)\n",
        "    \n",
        "    return correct_k\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l_dTKQblSzkm",
        "colab": {}
      },
      "source": [
        "lr_init = 0.01              # initial learning rate\n",
        "lr_factor = 0.1             # learning rate decay factor\n",
        "weight_decay_factor = 1e-4  # weight decay factor for L2 weight regularization\n",
        "lr_schedule_milestones = [90e3, 100e3, 110e3]\n",
        "\n",
        "# Define a Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer - SGD with momentum and weight_decay for L2 weight regularization\n",
        "#optimizer = torch.optim.SGD(net.parameters(), lr=lr_init, momentum=0.9, weight_decay=weight_decay_factor)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "\n",
        "if flag_regularize:\n",
        "    #optimizer = torch.optim.SGD(net.parameters(), lr=lr_init, momentum=0.9, weight_decay=weight_decay_factor)\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.01, weight_decay=weight_decay_factor)\n",
        "    \n",
        "    \n",
        "# Define learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=lr_schedule_milestones, gamma=lr_factor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BeHQMMqsS9Cz",
        "colab": {}
      },
      "source": [
        "# Function that takes a list of losses and plots them.\n",
        "def plot_losses(loss_list, steps):\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(pl.gcf())\n",
        "    pl.plot(steps, loss_list, c='b')\n",
        "    time.sleep(1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JQBpZRP-TKI0"
      },
      "source": [
        "### Define training parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YNtIcyk7S_ub",
        "colab": {}
      },
      "source": [
        "# Define number of training iterations and reporting intervals\n",
        "TRAIN_ITERS = 100e3  # @param\n",
        "REPORT_TRAIN_EVERY = 20  # @param\n",
        "PLOT_EVERY = 100  # @param\n",
        "REPORT_TEST_EVERY = 50  # @param\n",
        "TEST_ITERS = 100  # @param"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6ARGcbSGTFDN"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VSuV2dF-TDCD",
        "colab": {},
        "outputId": "845baf91-20b2-4a2f-927d-bd3d85d1b2d0"
      },
      "source": [
        "# Question: What is the accuracy of the model at iteration 0, i.e. before training starts?\n",
        "\n",
        "EPOCHS = int(TRAIN_ITERS / (TRAIN_DATASET_SIZE / BATCH_SIZE_TRAIN))\n",
        "\n",
        "train_iter = 0\n",
        "test_ct = 0\n",
        "\n",
        "losses = []\n",
        "steps = []\n",
        "\n",
        "test_data_provider = loopy_test_loader(test_loader)\n",
        "\n",
        "# set model in train mode\n",
        "net.train()\n",
        "\n",
        "running_loss = 0.0\n",
        "\n",
        "for epoch in range(int(EPOCHS)):  # loop over the dataset multiple times\n",
        "    \n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # set the learning rate and decay according to iteration schedule\n",
        "        lr_scheduler.step()\n",
        "        \n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if train_iter % REPORT_TRAIN_EVERY == REPORT_TRAIN_EVERY - 1:    # print every REPORT_TRAIN_EVERY mini-batch iterations\n",
        "            train_loss = running_loss / train_iter\n",
        "            \n",
        "            print('[%d, %5d, %6d] LR: %.5f' % (epoch + 1, i + 1, train_iter, lr_scheduler.get_lr()[-1]))\n",
        "            print('[%d, %5d] loss: %.5f' %\n",
        "                  (epoch + 1, i + 1, train_loss))\n",
        "            \n",
        "            losses.append(train_loss)\n",
        "            steps.append(train_iter)\n",
        "            \n",
        "        if train_iter % PLOT_EVERY == 0:\n",
        "            plot_losses(losses, steps)\n",
        "            \n",
        "        train_iter += 1\n",
        "    \n",
        "        if train_iter % REPORT_TEST_EVERY == 0:\n",
        "            # set model in test mode\n",
        "            net.eval()\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                # evaluate over at most TEST_ITER sub samples from the test_loader\n",
        "                test_iter = 0\n",
        "                test_loss = 0\n",
        "                correct = 0\n",
        "                \n",
        "                while test_iter < TEST_ITERS:\n",
        "                #for j, test_data in enumerate(test_loader, start=test_ct):\n",
        "                    test_data = next(test_data_provider)\n",
        "                        \n",
        "                    # get the test inputs; data is a list of [inputs, labels]\n",
        "                    test_inputs, test_labels = test_data[0].to(device), test_data[1].to(device)\n",
        "                    \n",
        "                    out = net(test_inputs)\n",
        "                    test_loss += criterion(out, test_labels)\n",
        "                    \n",
        "                    correct += top_k_accuracy(1, test_labels, out)\n",
        "                    \n",
        "                    test_iter += 1\n",
        "                    \n",
        "                avg_test_loss = test_loss / TEST_ITERS\n",
        "                avg_acc = correct / TEST_ITERS\n",
        "                \n",
        "                print('[%d, %5d] avg_test_loss: %.5f, avg_test_acc: %.2f' \n",
        "                    % (epoch + 1, i + 1, avg_test_loss, avg_acc))\n",
        "                \n",
        "                # next time start from farther in the test_loader\n",
        "                #test_ct += test_iter\n",
        "                \n",
        "                #if test_ct >= TEST_DATASET_SIZE:\n",
        "                #    # reset if having reached end of test dataset\n",
        "                #    test_ct = 0\n",
        "                \n",
        "            # set model back in train mode\n",
        "            net.train()\n",
        "    \n",
        "print('Finished Training')\n",
        "\"\"\"\n",
        "for train_iter in range(int(TRAIN_ITERS)):\n",
        "  _, train_loss_np, inp_img, tr_lbl = sess.run([training_op, train_loss, inp_train, batch_train_labels])\n",
        "  \n",
        "  if (train_iter % REPORT_TRAIN_EVERY) == 0:\n",
        "    losses.append(train_loss_np)\n",
        "    steps.append(train_iter)\n",
        "  if (train_iter % PLOT_EVERY) == 0:\n",
        "    pass\n",
        "    # plot_losses(losses, steps)    \n",
        "    \n",
        "  if (train_iter % REPORT_TEST_EVERY) == 0:\n",
        "    avg_acc = 0.0\n",
        "    train_avg_acc = 0.0\n",
        "    for test_iter in range(TEST_ITERS):\n",
        "      acc, acc_train = sess.run([test_acc_op, train_acc_op])\n",
        "      avg_acc += acc\n",
        "      train_avg_acc += acc_train\n",
        "      \n",
        "    avg_acc /= (TEST_ITERS)\n",
        "    train_avg_acc /= (TEST_ITERS)\n",
        "    print ('Test acc at iter {0:5d} out of {1:5d} is {2:.2f}%'.format(int(train_iter), int(TRAIN_ITERS), avg_acc*100.0))\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-17f9a6d3bb95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain_iter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mPLOT_EVERY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mplot_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mtrain_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-b74dea6c6aac>\u001b[0m in \u001b[0;36mplot_losses\u001b[0;34m(loss_list, steps)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHOL_pDk-XnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKw4zNL--XnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}